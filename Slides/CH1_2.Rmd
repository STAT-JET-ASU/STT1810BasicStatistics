---
title: "STT1810 --- Section 1.2<br>Strength of Evidence #1"
subtitle: "Introduction to Statistical Investigations, 2^nd^ Edition"
author: "Author: Jill E. Thomley // Appalachian State University"
date: "Last updated `r format(Sys.time(), '%A, %B %d, %Y @ %I:%M %p')`"
output: 
  ioslides_presentation:
    logo: images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```


## Recall: Spiral Approach & Four Pillars

:::::: {style="display: flex;"}

::: {}

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/sixstepsfourpillars.png", dpi = 100)
```

:::

::: {}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

:::

::: {}

* &#187; We will address *strength* of evidence (**significance**) in Chapter 1. &#171;

* We will discuss *breadth* of results (**generalization**) in Chapter 2.

* We will learn about *size* for an effect (**estimation**) in Chapter 3.

* Chapter 4 will introduce the idea of **causation**, the last of the four pillars. 

:::

::::


## Learning Goals for Chapter 1

* Use the chance model to determine whether an observed statistic is unlikely to occur. [Section 1.1]
* Calculate and interpret a *p*-value, and state the strength of evidence it provides against the null hypothesis. [Section 1.2]
* Calculate a standardized statistic for a single proportion and evaluate the strength of evidence it provides against a null hypothesis. [Section 1.3]
* Describe how the distance of the observed statistic from the parameter value specified by the null hypothesis, sample size, and one- vs. two-sided tests affect the strength of evidence against the null hypothesis. [Section 1.4]

We will *not* be covering Section 1.5, which discusses the theory-based approach to inference for a single proportion.


## Learning Goals for Section 1.2

* Use appropriate symbols for parameter and statistic.
* State the null and the alternative hypotheses in words and in terms of the symbol $\pi$, the long-run proportion.
* Explain how to conduct a simulation using a null hypothesis probability that is not 50-50.
* Use the [ISI One Proportion applet](http://www.isi-stats.com/isi2nd/ISIapplets2021.html) to obtain the *p*-value after carrying out an appropriate simulation.
* Anticipate the null distribution's center and how it changes based on using proportion or count as the statistic.
* Interpret the *p*-value and explain why a smaller *p*-value provides stronger evidence against the null hypothesis.
* State a conclusion about the alternative hypothesis and null hypothesis based on the *p*-value.


## Section 1.2 New Vocabulary

**binary variable:** a categorical variable with only two outcomes

* can regroup multiple outcomes into two categories
* one of the groups would be defined to be a “success” 
* the other group would be defined to be a “failure”

***Example:*** The outcome of one coin flip (heads vs. tails) is a very common binary variable. In a pre-game coin flip in football, one team designates what will be a "success" by calling heads or tails before the flip.

***Example:*** The six possible numbers resulting from one roll of a six-sided die can be regrouped into odd vs. even numbers. We could then designate evens as the "success" if that is what our research question is interested in.


##

An essential part of our statistical *logic of inference* is creating a pair of opposing hypotheses that make statements regarding a parameter of the random process we are investigating.

*Recall from our Section 1.1 vocabulary: A parameter is a long-run numerical property of some real-world random process.*

**null hypothesis ($H_0$):** "by chance alone" or "there is no effect" explanation, which can be modeled by random simulation

**null distribution:** a distribution of simulated statistics that are a representation of what *could have* happened in the actual study if the null hypothesis were true (we *assume* $H_0$ is true when we conduct a simulation)

**alternative hypothesis ($H_a$):** "not by chance alone" or "there is an effect" explanation, typically reflects our research conjecture (it is also sometimes referred to as the *research hypothesis*)


## Symbols

We often use symbols to communicate or represent different quantities in a statistical analysis or to express our hypotheses. 

It is common to use Greek letters to represent parameters and Roman letters (our regular English alphabet) for statistics.

$\pi$ (pi): The Greek letter for p, pronounced “pie.” This represents a parameter that is a proportion or probability (*not* 3.14159).

$\hat{p}$: The Roman letter p with a ^ above it, pronounced as "p-hat." It is the proportion of observational units in a sample that have a particular characteristic (or an *estimated probability*). For a *test of significance*, this would be the observed statistic.

$n$: sample size; the number of observational units in the sample


## Test of Significance

In the field of Statistics, a *test of significance* is a procedure for measuring the *strength of evidence* against our null hypothesis regarding the parameter of interest.

(1) make a claim about the parameter of interest ($H_0$ and $H_a$)
(2) gather and explore data
(3) follow the <u>3S</u> strategy to 
    * calculate an observed <u>statistic</u> from the data
    * <u>simulate</u> a null distribution (*scenario assuming $H_0$ is true*)
    * compare the observed statistic to the null distribution to measure the <u>strength of evidence</u> it provides *against* $H_0$.  
(4) draw a conclusion about the plausibility of $H_0$


## Quantifying Strength of Evidence

One way to assess strength is to use a *p*-value. The **p** stands for *probability*---it is the probability of getting a purely random value of the statistic (e.g., from simulation) that is at least as extreme (*in the direction of $H_a$*) as the observed statistic when $H_0$ is true.

| Magnitude               | Guideline for Inference                                        |
|-------------------------|----------------------------------------------------------------|
| *p*-value > 0.10        | not much evidence against the null hypothesis; $H_0$ is plausible |
| 0.05 < *p*-value ≤ 0.10 | moderate evidence against the null hypothesis                  |
| 0.01 < *p*-value ≤ 0.05 | strong evidence against the null hypothesis                    |
| *p*-value ≤ 0.01        | very strong evidence against the null hypothesis               |


## Additional Guidelines (0.05)

Researchers across a variety of disciplines consider a *p*-value less than or equal to 0.05 (i.e., at most a 5% chance) to be the threshold where we can claim data have *statistical significance* and declare the chance model ($H_0$) to be implausible. 

A *p*-value ≤ 0.05 corresponds to *strong* or *very strong evidence* against the null hypothesis in our textbook guidelines.

However, in some situations you may want stronger evidence. For example, in medicine, rejecting $H_0$ may have life or death consequences, so researchers could choose to use a value like 0.01 (*very strong evidence*) instead when assessing data.

Why 0.05? It's honestly an arbitrary value. Ronald Fisher wrote *Statistical Methods for Research Workers* in 1925 and used 1/20 (5%) as his general guide; it has propagated over time.


## [ISI One Proportion Applet](http://www.isi-stats.com/isi2nd/ISIapplets2021.html)

:::::: {style="display: flex;"}

::: {}

* Probability of heads = value of $\pi$ in $H_0$

* Number of tosses = $n$ (sample size)

* Number of repetitions = 1000 (or more simulations)

* Proportion of heads = $\hat{p}$

:::

::: {}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

:::

::: {}

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/OneProportionApplet.png", dpi = 120)
```

:::

::::


## Flashback: [ESP Test](https://stat-jet-asu.github.io/Slides/STT1810/ESPZenerCards.html)

***Example:*** If we did a test where someone was presented with 25 cards to guess, how many would they have to get right to make you get suspicious that “something else is going on” besides just random guessing? We can now formally test this.

Let $\pi$ be the long-term probability of correctly identifying a card.

$$H_0: \pi = 0.20 \text{ (guessing)}$$ 

$$H_a: \pi > 0.20 \text{ (has ESP)}$$

Suppose I correctly identified 9 cards, so my sample proportion is $\hat{p} = 9/25 = 0.36.$

What *strength of evidence* is provided by these ESP test results?


## Simulate & Measure

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/OneProportionAppletZenerB.png", dpi = 140)
```


## Strength of Evidence/Conclusions

The *p*-value from my simulation is 0.0490. This falls in the range of 0.01 < *p*-value ≤ 0.05, which indicates strong evidence against the null hypothesis. The results are statistically significant: that is, unlikely to be due to chance alone. The chance model is *not plausible* in this case. We will conclude $H_a$ --- I have ESP!

| Magnitude               | Guideline for Inference                                           |
|-------------------------|-------------------------------------------------------------------|
| *p*-value > 0.10        | not much evidence against the null hypothesis; $H_0$ is plausible |
| 0.05 < *p*-value ≤ 0.10 | moderate evidence against the null hypothesis                     |
| 0.01 < *p*-value ≤ 0.05 | strong evidence against the null hypothesis                       |
| *p*-value ≤ 0.01        | very strong evidence against the null hypothesis                  |


## Generalization

*Okay, so maybe I don't have ESP... *

From a probability point of view, we *did* reject the plausibility of the chance model, but the breath of our conclusions are limited by the nature of the experiment and our sample.

It's still *possible* that the results were due to random chance and we drew an incorrect decision based on the data.

If our data and *test of significance* process lead us to declare our results to be statistically significant and we reject $H_0$ (chance model) as a plausible explanation when the study outcomes are indeed due solely to chance, that is referred to as a *Type I Error*. We will discus this more in Section 1.4.

Look back and ahead: how can we improve our data collection?


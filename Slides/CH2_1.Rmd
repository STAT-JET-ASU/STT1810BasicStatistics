---
title: "STT1810 --- Section 2.1<br>Sampling Distributions"
subtitle: "Introduction to Statistical Investigations, 2^nd^ Edition"
author: "Author: Jill E. Thomley // Appalachian State University"
date: "Last updated `r format(Sys.time(), '%A, %B %d, %Y @ %I:%M %p')`"
output: 
  ioslides_presentation:
    logo: images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```


## Learning Goals for Chapter 2

We will do only Section 2.1: Sampling From a Finite Population. It introduces the idea of the *sampling distribution* of $\hat{p}$ built from a large number of *random samples* of size $n$ drawn from a process with parameter $\pi$. Also, the *Central Limit Theorem*.

<hr>

* Describe how to select a random sample.
* Understand and predict sampling distributions for a sample proportion.

<hr>

This material builds on the Preliminaries and Chapter 1. You will need to remember and use vocabulary and concepts from those sections to understand the new material. 

Section 2.1 provides a foundation for the upcoming Section 3.2.


## Learning Goals for Section 2.1

* Identify parameters (long-run proportion) and statistics (sample proportion) in a statistical study.
* Identify which statistics (proportions) and graphs (bar graph) are appropriate for categorical variables, construct graphs and calculate statistics with use of technology, and interpret appropriately.
* Identify the (finite) population and the sample in a statistical study.
* Identify whether a sampling method is likely to be biased and explain the potential impact.
* Describe how to select a random sample and recognize that one advantage of a random sample is that it is likely to be representative of the population regardless of sample size.


## *More* Learning Goals for Section 2.1

* Fill in a data table where rows are the observational units and columns are the variables.
* Predict the mean, standard deviation, and shape of the sampling distribution of a sample proportion from a random sample of size n, where the population proportion  is known.
* Apply simulation-based inference methods for a population proportion to research studies involving random samples from finite populations.
* Identify whether a study may be impacted by non-sampling concerns and explain the potential impact.

*This chapter is a little more theoretical/conceptual. We will need it going forward to understand some of the upcoming methods. One important idea is generalizing beyond the sample (**breadth**).*


## Section 2.1 New Vocabulary

* bias
* census
* nonsampling concerns
* population
* sample (*recall, this can be an adjective, noun, or verb*)
  * convenience sample
  * random sample
  * simple random sample
  * sampling frame
* sampling distribution / Central Limit Theorem
* sampling variability


## 

Recall *parameter*, *statistic*, and the other vocabulary terms we learned in Chapter 1 and the Preliminaries sections. Review as needed to make sure you have the meanings solidified, so that you can use statistical language effectively as we move on.

**population:** the entire collection of observational units we are interested in (*as determined by the research question*)

**census:** when we collect data on all individuals in a population (*this is the intention of the decennial Census in the U.S.*)

**sample:** a subset of a population on which we actually observe and record data

*There are many ways to choose samples; some methods are better.*

**convenience sample:** non-random subset of a population (e.g., voluntary response, like a phone-in or mail-in survey)


##

**random sample:** a subset of observational units chosen using a probability device or method (*not all units have to have an equal chance to be chosen, just a known probability of being chosen*)

**simple random sample:** a subset of observational units chosen so that every sample of a given size *n* is equally likely to be the sample that we select from the population (*each observational unit has the same chance of being chosen as every other unit*)

<hr> 

A sampling method is considered to be ***biased*** if the statistics calculated from different samples consistently overestimate or consistently underestimate the population parameter in which we are interested. *Non-random samples tend to be biased.*

<hr>

**nonsampling concerns:** potential sources of bias (e.g., poorly worded survey questions) that are not related to the way the sample was selected from the population


## 

**sampling frame:** a roster or "master list" of every observational unit in a given population of interest; we need this in order to be able to choose a random sample

**sampling variability:** the amount that a statistic changes from sample to sample as we repeatedly observe it (*take a bunch of samples and calculate the same statistic---how different are they?*)

**sampling distribution:** distribution of a statistic for all possible samples of size $n$ randomly selected from the same population

<hr>

The ***Central Limit Theorem*** tells us that when we sample from a large finite population, the sampling distribution of the sample proportion $\hat{p}$ calculated from from repeated random samples will be approximately normal if there are at least 10 successes and at least 10 failures in each sample.

<hr>


## *So* ... What is a Sampling Distribution?

In Section P.2 you learned about the distribution of a variable, which is described using shape, center, spread/variability, and unusual observations/outliers.

For a sampling distribution, the "variable" is a statistic of some kind, like the sample proportion $\hat{p}$. 

If we take many, many random samples of the same size from a population with parameter $\pi$, the value of the statistic $\hat{p}$ will be different from sample to sample, but in a way we can predict *in the long run* from its distribution.

<hr>

*Understanding how statistics from different samples behave helps us develop reliable methods for statistical inference.*


## Key Central Limit Theorem Ideas

The Central Limit Theorem describes the sampling distributions of certain statistics, like $\hat{p}$, if they meet certain criteria... 

We have a large, finite population with parameter $\pi$. Here, *finite* means it is possible to count the observational units.

We are taking samples of size $n$ that are large enough that there are at least 10 successes and 10 failures in each sample.

We are calculating the sample proportion $\hat{p}$ for each sample.

**shape:** the sampling distribution will be approximately normal (*unimodal and mound-shaped---the classic "bell curve"*).

<hr>

&rarr; *Recall that we have observed this in many of our simulations...*


## 

**center:** The mean of the sampling distribution will be equal to $\pi$ (i.e., *all of the possible sample proportions $\hat{p}$ will average out to the population proportion---it is what we will call unbiased*).

**spread/variability:**

The standard deviation of the sampling distribution of $\hat{p}$ will be

$$SD(\hat{p}) = \sqrt{\frac{\pi(1-\pi)}{n}}.$$

Notice that $n$ is in the fraction denominator. For bigger sample sizes, SD will be smaller! *We have seen this before, when we did our simulations. Larger samples tend to have less variability and converge toward the long-run (population) value.*

Note: We call the SD of a sampling distribution its *standard error*.


## Two Example Calculations

Plug in the appropriate values and solve.

<hr>

For $\pi = 0.50$ and $n = 100$ ...

$$SD(\hat{p}) = \sqrt{\frac{.50(1-.50)}{100}} = \sqrt{\frac{0.50 \times 0.50}{100}} = 0.05$$

<hr>

For $\pi = 0.10$ and $n = 100$ ...

$$SD(\hat{p}) = \sqrt{\frac{.10(1-.10)}{100}} = \sqrt{\frac{0.10 \times 0.90}{100}} = 0.03$$

<hr>


##

Simulating a distribution using the [ISI One-Proportion Applet](https://www.rossmanchance.com/applets/2021/oneprop/OneProp.htm)...

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/SamplingDist50.png", dpi = 100)
```

<center>$\text{probability of success}(\pi) = 0.50$<br>
$\text{sample size}(n) = 100$</center>


##

Simulating a distribution using the [ISI One-Proportion Applet](https://www.rossmanchance.com/applets/2021/oneprop/OneProp.htm)...

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("images/SamplingDist10.png", dpi = 100)
```

<center>$\text{probability of success}(\pi) = 0.10$<br>
$\text{sample size}(n) = 100$</center>


## Different Sample Size, Same $\pi$

| $n$    |  $\text{Mean}(\hat{p}) = \pi$  |  $SD(\hat{p})$  |
|:------:|:------------------------------:|:---------------:|
|  25    |             0.50               |     0.1000      |
|  50    |             0.50               |     0.0707      |
|  100   |             0.50               |     0.0500      |
|  200   |             0.50               |     0.0354      |
|  400   |             0.50               |     0.0250      |
|  800   |             0.50               |     0.0177      |
|  1000  |             0.50               |     0.0158      |


## Same Sample Size, Different $\pi$

| $n$   |  $\text{Mean}(\hat{p}) = \pi$  |  $SD(\hat{p})$  |
|:-----:|:------------------------------:|:---------------:|
|  100  |              0.10              |     0.0300      |
|  100  |              0.25              |     0.0433      |
|  100  |              0.40              |     0.0490      |
|  100  |              0.50              |     0.0500      |
|  100  |              0.60              |     0.0490      |
|  100  |              0.75              |     0.0433      |
|  100  |              0.90              |     0.0300      |


## Type of Sampling Matters!

* Population size does not affect sampling variability as long as the population is at least 20 times the size of the sample.

* A larger sample size is not helpful if the sampling method is biased. You essentially just get a larger amount of *bad* data (as well as a false sense of confidence in your findings).

* If the sample we have is biased, we have to be careful about generalizing to the population the sample came from.

* Bias can get into our data in from both the sampling method (e.g., convenience samples) and nonsampling sources (e.g., bad survey questions).

* Statistical methods are designed to handle *random sampling variability* in our data.


## *So* ... What's Different from Chapter 1?

In Chapter 1, you were generating hypothetical samples from a probability model to reflect our null hypothesis Ho. In that case, $\pi$ is a hypothetical value assumed in Ho.

In a test of significance, we use the hypothetical $\pi$ to try to make an inference about an actual population $\pi$.

Now, we are taking samples from an actual population. Why? So we can explore what kinds of samples we might get. In this case, $\pi$ is an actual population value.

<hr>

We will use the similarity between the two ideas in Section 3.1 to estimate a population $\pi$ using a *confidence interval*. We will build on the idea of sampling distributions in Section 3.2.


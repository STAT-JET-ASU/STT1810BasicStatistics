---
title: "Section 1.1 Introduction<br>to Chance Models"
subtitle: "*Introduction to Statistical Investigations, 2^nd^ Edition*"
author: "Jill E. Thomley | **STT 1810 BASIC STATISTICS** | Appalachian State University"
date: "Last updated `r format(Sys.time(), '%A, %B %d, %Y @ %I:%M %p')`"
output: 
  ioslides_presentation:
    logo: ../images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```


## Before We Beginâ€¦

These slides were created to accompany the text [*Introduction to Statistical Investigations, 2nd Edition*](http://www.isi-stats.com/isi/index2nd.html) --- Section 1.1 Introduction to Chance Models. 

*This content does not replace reading the relevant textbook section.* It is for class presentation, review, and reference.

See [AsULearn](https://asulearn.appstate.edu/) for additional readings, videos, and assignments.

You can print these slides to a pdf for offline use using the print function in your browser.


## 

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/statisticsgrammar.jpg", dpi = 160)
```

*Grammar gives us rules that are essential for any shared language.* 

Statistics provides frameworks and methods to help make sense of the complex language of data. Thus, researchers can ***quantify uncertainty***, ***validate hypotheses***, and ***draw valid conclusions***. 


## Recall: Spiral Approach & Four Pillars

:::::: {style="display: flex;"}

::: {}

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/sixstepsfourpillars.png", dpi = 100)
```

:::

::: {}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

:::

::: {}

* **We will address strength of evidence (*significance*) in Chapter 1.**

* We will discuss breadth of results (*generalization*) in Chapter 2.

* We will learn about effect size of results (*estimation*) in Chapter 3.

* Chapter 4 will introduce the idea of *causation*, the last of the four pillars. 

:::

::::


## Learning Goals for Chapter 1

* Use the chance model to determine whether an observed statistic is unlikely to occur. [Section 1.1]
* Calculate and interpret a *p*-value, and state the strength of evidence it provides against the null hypothesis. [Section 1.2]
* Calculate a standardized statistic for a single proportion and evaluate the strength of evidence it provides against a null hypothesis. [Section 1.3]
* Describe how the distance of the observed statistic from the parameter value specified by the null hypothesis, sample size, and one- vs. two-sided tests affect the strength of evidence against the null hypothesis. [Section 1.4]

<hr>

We will not cover Section 1.5, which introduces a theory-based method of inference for a single proportion.


## Learning Goals for Section 1.1 

* Recognize the difference between parameters and statistics.
* Describe how to use coin tossing to simulate outcomes from a chance model of a random choice between two events.
* Use the ISI [One Proportion](http://www.isi-stats.com/isi2nd/ISIapplets2021.html) applet to carry out a coin tossing simulation.
* Identify whether or not the results of a study are statistically significant and whether or not a chance model is a plausible explanation for the data.
* Implement the 3S strategy: find a statistic, simulate results from a chance model, and determine strength of evidence.
* Differentiate between saying a chance model is plausible and a chance model is the correct explanation for observed data (i.e., whether or not we have "proven" a result).


## What is a Chance Model?

<h3 style="text-align: center;">Generally speaking, what is a *model* ?</h3>

<h3 style="text-align: center;">So ... what is a *mathematical model* ?</h3>

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/molecularmodel.jpg", dpi = 110)
```

<h3 style="text-align: center;">Building on these two ideas, what is a *chance model* ?</h3>


## Textbook Definitions

**<mark>model</mark>** (mathematical) 

* A representation using math or probability tools that is meant to closely match reality, but always making *assumptions* about that reality, which may or may not be true.

**<mark>chance model</mark>**

* A real or computerized process to generate data according to a well-understood set of conditions." --- *to model some real-life random process we are interested in studying*

<hr> 

In the real world we are almost always dealing with incomplete data (i.e., *samples*). Therefore, in statistics, we logically compare outcomes we observe in the real world to hypothetical chance models. This is the foundation of *statistical inference*.


## Section 1.1 New Vocabulary

Study and learn these new terms so that you can communicate statistical concepts and results effectively.

* chance model
* parameter
* sample
* sample size
* simulate
* statistic
* statistically significant
* strength of evidence
* 3S strategy (*statistic, simulate, strength*)

## 

**<mark>chance model</mark> :** a physical / tactile or computerized process to generate random data according to a "well-understood" set of conditions&mdash;with some necessary assumptions

The model reflects important properties of the real-life random process we are studying. We use cards, coins, dice, and spinners to make models, as well as computer programs / applets.

***Example 1:*** For the ESP test we did, one condition was using an "open deck" (*sampling with replacement*) so that the probability of randomly guessing the correct symbol on each trial would stay the same. We *assumed* there was no other way for someone to know which card was drawn (e.g., marked cards, cheating).

***Example 2:*** For the Monty Hall game, we *assumed* that the prize was equally likely to be placed behind any of the three doors. In our extended version, another condition was that we opened all of the unchosen doors *except one* after the initial guess.


## 

**<mark>parameter</mark> :** a long-run numerical property of a random process

***Example 1:*** For the ESP example, the parameter is each person's innate chance of correctly identifying a card. That chance is 0.2 if they do not have ESP, versus *more than* 0.2 if they do have ESP. This probability is reflected in their *long-run* test performance---many more trials than 25. 

Remember, the ESP testing web site suggests that 50+ trials are needed to get "a reliable indication of your performance".

***Example 2:*** Across a large number of instances of the Monty Hall game, what is the probability a wins player if they switch? This is one scenario we investigated using our 1000+ simulations of the game using applets. 

The *long-run* probability of winning the game if you switch is the parameter for the random process.


##

**<mark>sample</mark> :** the set of observational units on which we collect data

***Example 1:*** One card presented for identification in our Zener card ESP test is an observational unit. The sample is the total number of cards a person attempts in their test.

***Example 2:*** Playing one round of the Monty Hall game is a unit; the number of rounds we play is the sample size.


**<mark>sample size</mark> :** the number of observational units ($n$) in a sample

***Example 1:*** For our ESP test, we attempted $n = 25$ cards. If we took the test twice under identical conditions, we could put the data together for a sample of $n = 50$.

***Example 2:*** We initially did $n = 15$ trials of the Monty Hall game. Later, we increased to $n = 100$ and $n = 1000$ trials.


##

**<mark>statistic</mark> :** a numerical value that summarizes results in a sample (i.e., results in the set of *observed data*)

The value of a sample *statistic* we get from observing a random process is a way to estimate the value of the process *parameter*.

***Example 1:*** During the ESP test, our statistic was the number of cards out of 25 that we correctly identified.

***Example 2:*** For Monty Hall, we recorded a win (car) or loss (goat) for each game we played, then we counted the total number of wins and calculated the fraction of wins. Both are statistics. 

**<mark>statistically significant</mark> :** unlikely to occur just by random chance 

In other words, the observed result *could* be solely a product of random chance, but it's not *likely* to be just random chance (i.e., random chance alone is not a *plausible* explanation).


## The 3S Strategy

This is a method for assessing the evidence provided by data.

**<mark>Statistic</mark> :** Compute a statistic from your observed sample data. 

**<mark>Simulate</mark> :** Identify a possible "by-chance-alone" explanation for the data. Repeatedly simulate values of the statistic that *could* happen when the chance model is true---purely random results.  

**<mark>Strength of evidence</mark> :** Compare your observed statistic to the simulated values. As you do this, consider whether the statistic from the sample data is *unlikely* when the chance model is true. 

If we decide the observed statistic is unlikely to occur by chance alone, we can logically conclude that the observed data provide us strong evidence against the *plausibility* (i.e., the *believability* or the *credibility*) of the chance model --- but *NOT* "proof". 


## Why Not Proof?

Just because we have not yet found evidence for something we are studying, that does not automatically prove it does not exist. When we use samples, there is always ***uncertainty*** (randomness) in our research findings&mdash;think *improbable* vs. *impossible*.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("https://stat-jet-asu.github.io/Moodlepics/admin/absenceofevidence.png")
```


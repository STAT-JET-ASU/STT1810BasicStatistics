---
title: "Section 1.4 What Impacts Strength of Evidence?"
subtitle: "*Introduction to Statistical Investigations, 2^nd^ Edition*"
author: "Jill E. Thomley | **STT 1810 BASIC STATISTICS** | Appalachian State University"
date: "Last updated `r format(Sys.time(), '%A, %B %d, %Y @ %I:%M %p')`"
output: 
  ioslides_presentation:
    logo: ../images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```


## Before We Begin…

These slides were created to accompany the text [*Introduction to Statistical Investigations, 2nd Edition*](http://www.isi-stats.com/isi/index2nd.html) --- Section 1.4 What Impacts Strength of Evidence? 

*This content does not replace reading the relevant textbook section.* It is for class presentation, review, and reference.

See [AsULearn](https://asulearn.appstate.edu/) for additional readings, videos, and assignments.

These slides also include supplemental content about Type I and Type II errors that is not found in Section 1.4 of the ISI textbook.


## Recall: Spiral Approach & Four Pillars

:::::: {style="display: flex;"}

::: {}

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/sixstepsfourpillars.png", dpi = 100)
```

:::

::: {}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

:::

::: {}

* **We will address strength of evidence (*significance*) in Chapter 1.**

* We will discuss breadth of results (*generalization*) in Chapter 2.

* We will learn about effect size of results (*estimation*) in Chapter 3.

* Chapter 4 will introduce the idea of *causation*, the last of the four pillars. 

:::

::::


## Learning Goals for Chapter 1

* Use the chance model to determine whether an observed statistic is unlikely to occur. [Section 1.1]
* Calculate and interpret a *p*-value, and state the strength of evidence it provides against the null hypothesis. [Section 1.2]
* Calculate a standardized statistic for a single proportion and evaluate the strength of evidence it provides against a null hypothesis. [Section 1.3]
* Describe how the distance of the observed statistic from the parameter value specified by the null hypothesis, sample size, and one- vs. two-sided tests affect the strength of evidence against the null hypothesis. [Section 1.4]

We will not cover Section 1.5, regarding theory-based inference.


## Learning Goals for Section 1.4

* Anticipate and explain why, when everything else remains the same, the *p*-value is smaller when the observed proportion of successes is farther away from the hypothesized value of the long-run proportion of successes.
* Anticipate and explain why, when everything else remains the same, the *p*-value is smaller when the sample size is larger.
* Recognize when a two-sided test / alternative hypothesis is suggested by the research question.
* Anticipate and explain why, when everything else remains the same, the *p*-value is larger when the alternative hypothesis is two-sided.


## Section 1.4 New Vocabulary

**<mark>two-sided test / alternative hypothesis</mark> :** a test of significance in which the alternative hypothesis Ha has the form "not equal to" or "does not equal" (≠) rather than less or greater (< or >). 

***Example:*** Our online ESP test mentions both "psi-hitting" (getting more cards correct than expected by chance) and "psi-missing" (getting fewer correct). If we are interested in either of those two possibilities, we would write our hypotheses as follows.

$$H_0: \pi = 0.20$$

$$H_a: \pi \neq 0.20$$

In this case, "extreme" would be measured on *both* sides of the null distribution, not just the upper side like $H_a: \pi > 0.20$.


## What Impacts Strength of Evidence?

Three important components of a test of significance influence the strength provided by the data evidence.

* **<mark>extremeness</mark> :** The farther the observed statistic $\hat{p}$ is from the average value of the null distribution (i.e., the value of $\pi$ given in $H_0$), the more evidence there is against $H_0$.

* **<mark>sample size</mark> :** As the sample size increases (while the value of the observed sample statistic $\hat{p}$ and our hypotheses stay the same), the strength of evidence against $H_0$ increases.

* **<mark>one- vs. two-sided $H_a$</mark> :** Because the *p*-value for a two-sided test is about twice as large as for a one-sided test using the same data, it provides less evidence against $H_0$. (Two-sided tests are used more often in scientific practice.)


## Type I & Type II Errors

Our estimated *p*-values are probabilities, not proof. When we do a significance test, we might make a decision error. For example, a small sample size is a common reason for a Type II error.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/errortypes.png", dpi = 225)
```
<p style="text-align:center; font-size: 10px;">Source: [Deepak Chopra | Talking Data Science](https://medium.com/@deepakchopra2911) on Medium</p>


## Type I Error and Significance

A **<mark>Type I error</mark>** happens when you conclude that your results are statistically significant when, in reality, they happened purely by chance ($H_0$ is really true). Essentially, you had the misfortune of getting a random result that is *improbable* but not *impossible*.

The risk of committing this error is the significance level (α) you choose. That is the value that you set at the start of your study as the decision criteria for a *p*-value, like saying strong evidence is a *p*-value ≤ 0.05 (or *z*-score > 2). Significance level is commonly set at 0.05 (5%), so this is our usual chance of Type I error.

A **<mark>Type II error</mark>** happens when $H_0$ is genuinely false and we fail to achieve statistically significant results from our data (i.e., we decide $H_0$ is plausible). The chance depends on many factors, including sample size and how close a false $H_0$ is to the truth. 


## Example of Type I & II Errors

You decide to get tested for COVID-19 based on mild symptoms. There are two correct decisions and two errors that could occur. Here, the null hypothesis is that you *do not* have COVID-19.

The "unknown truth" is : You really DO NOT have COVID-19.

* Test says you do not have the virus---yes, a correct decision!
* Test says you do have the virus---*false positive* (Type I error)

The "unknown truth" is : You really DO have COVID-19.

* Test says you do have the virus---again, a correct decision!
* Test says you do not have the virus---this is a *false negative* (Type II error)



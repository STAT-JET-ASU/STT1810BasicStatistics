---
title: "Section 1.4 What Impacts Strength of Evidence?"
subtitle: "*Introduction to Statistical Investigations, 2^nd^ Edition*"
author: "Jill E. Thomley | **STT 1810 BASIC STATISTICS** | Appalachian State University"
date: "Last updated `r format(Sys.time(), '%A, %B %d, %Y @ %I:%M %p')`"
output: 
  ioslides_presentation:
    logo: ../images/logoASU.jpg
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```


## Before We Begin…

These slides are meant to accompany [*Introduction to Statistical Investigations, 2nd Edition*](http://www.isi-stats.com/isi/index2nd.html) --- Section 1.4 What Impacts Strength of Evidence? 

*This content does not replace reading the textbook section.* It is for class presentation and/or reference in STT 1810 Basic Statistics.

See [AsULearn](https://asulearn.appstate.edu/) for supplemental readings, videos, assignments, and searchable Glossary of ISI Textbook Vocabulary.

These slides also include supplemental information on Type I and Type II errors that is not found in Section 1.4.


## Recall: Spiral Approach & Four Pillars

:::::: {style="display: flex;"}

::: {}

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/sixstepsfourpillars.png", dpi = 100)
```

:::

::: {}

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

:::

::: {}

* **We will address strength of evidence (*significance*) in Chapter 1.**

* We will discuss breadth of results (*generalization*) in Chapter 2.

* We will learn about effect size of results (*estimation*) in Chapter 3.

* Chapter 4 will introduce the idea of *causation*, the last of the four pillars. 

:::

::::


## Learning Goals for Chapter 1

* Use the chance model to determine whether an observed statistic is unlikely to occur. [Section 1.1]
* Calculate and interpret a *p*-value, and state the strength of evidence it provides against the null hypothesis. [Section 1.2]
* Calculate a standardized statistic for a single proportion and evaluate the strength of evidence it provides against a null hypothesis. [Section 1.3]
* Describe how the distance of the observed statistic from the parameter value specified by the null hypothesis, sample size, and one- vs. two-sided tests affect the strength of evidence against the null hypothesis. [Section 1.4]

We will not cover Section 1.5, regarding theory-based inference.


## Learning Goals for Section 1.4

* Anticipate and explain why, when everything else remains the same, the *p*-value is smaller when the observed proportion of successes is farther away from the hypothesized value of the long-run proportion of successes.
* Anticipate and explain why, when everything else remains the same, the *p*-value is smaller when the sample size is larger.
* Recognize when a two-sided test / alternative hypothesis is suggested by the research question.
* Anticipate and explain why, when everything else remains the same, the *p*-value is larger when the alternative hypothesis is two-sided.


## Section 1.4 New Vocabulary

**two-sided test / alternative hypothesis:** a test of significance in which the alternative hypothesis Ha has the form "not equal to" or "does not equal" (≠) rather than less or greater (< or >). 

***Example:*** Our online ESP test mentions both "psi-hitting" (getting more cards correct than expected by chance) and "psi-missing" (getting fewer correct). If we are interested in either of those two possibilities, we would write our hypotheses as follows.

$$H_0: \pi = 0.20$$

$$H_a: \pi \neq 0.20$$

In this case, "extreme" would be measured on *both* sides of the null distribution, not just the upper side like $H_a: \pi > 0.20$.


## What Impacts Strength of Evidence?

Three important components of a test of significance influence the strength provided by the data evidence.

* **extremeness:** The farther the observed statistic $\hat{p}$ is from the average value of the null distribution (i.e., the value of $\pi$ given in $H_0$), the more evidence there is against $H_0$.

* **sample size:** As the sample size increases (while the value of the observed sample statistic $\hat{p}$ and our hypotheses stay the same), the strength of evidence against $H_0$ increases.

* **one- vs. two-sided $H_a$:** Because the *p*-value for a two-sided test is about twice as large as for a one-sided test using the same data, it provides less evidence against $H_0$. (Two-sided tests are used more often in scientific practice.)


## Example: Recall Our [ESP Test](https://stat-jet-asu.github.io/STT1810BasicStatistics/Slides/ChapterXX/ESPZenerCards.html)

***Example:*** We can use the [One Proportion](http://www.isi-stats.com/isi2nd/ISIapplets2021.html) applet to assess the statistical significance of our Zener card ESP test results. 

$$H_0: \pi = 0.20$$

$$H_a: \pi > 0.20$$

Let's say my results were 6 out of 25 cards correct.

* What is the *p*-value for the result of 6 out of 25 correct?
* What if I had done a test with 50 cards and got 12 right?
* What if I had scored 9 out of 25 correct instead of just 6?
* What if I had tested for both psi-hitting and psi-missing? 


## 

**INITIAL:** I got 6 of 25 cards correct on my ESP test, so $\hat{p} = 0.24$.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/OneProportionAppletZenerA.png", dpi = 150)
```


## 

**Larger Sample:** I got 12 of 50 cards correct, so $\hat{p}$ is still $0.24$, but now $n = 50$. The *p*-value is smaller (stronger evidence).

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/OneProportionAppletZenerC.png", dpi = 150)
```


## 

**Larger Proportion:** I got 9 of 25 cards correct, so $n$ is still $25$, but now $\hat{p} = 0.36$. The *p*-value is smaller (stronger evidence).

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/OneProportionAppletZenerB.png", dpi = 150)
```


## 

**Two-Sided Hypothesis:** I used a *two*-sided alternative $H_a$ vs. a *one*-sided alternative. The *p*-value is larger (weaker evidence).

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/OneProportionAppletZenerD.png", dpi = 150)
```


## Summary of Results

Each case changes one input from the baseline test. The table shows the predicted increases or decreases in the *p*-values. We always have the same $H_0$.

| null             | alternative         | $n$ | $\hat{p}$ | *p*-value | change                 |
|:----------------:|:-------------------:|:---:|:---------:|:---------:|:----------------------:|
| $H_0:\pi = 0.20$ | $H_a:\pi > 0.20$    | 25  | 0.24      | 0.3810    | (*initial test*)       |
| $H_0:\pi = 0.20$ | $H_a:\pi > 0.20$    | 50  | 0.24      | 0.2850    | larger sample $n$      | 
| $H_0:\pi = 0.20$ | $H_a:\pi > 0.20$    | 25  | 0.36      | 0.0490    | more extreme $\hat{p}$ |
| $H_0:\pi = 0.20$ | $H_a:\pi \neq 0.20$ | 25  | 0.24      | 0.6173    | two-sided $H_a$        |

Remember: Smaller *p*-values provide *more* evidence against $H_0$.


## Type I & Type II Errors

Our estimated *p*-values are probabilities, not proof. When we do a significance test, we might make a decision error. For example, a small sample size is a common reason for a Type II error.

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("../images/errortypes.png", dpi = 225)
```
<p style="text-align:center; font-size: 10px;">Source: [Deepak Chopra | Talking Data Science](https://medium.com/@deepakchopra2911) on Medium</p>


## Example of Type I & II Errors

You decide to get tested for COVID-19 based on mild symptoms. There are two correct decisions and two errors that could occur. Here, the null hypothesis is that you *do not* have COVID-19.

Unknown Truth: You DO NOT have COVID-19.

* Test says you do have the virus---*false positive* (Type I error)
* Test says you do not have the virus---yes, a correct decision!

Unknown Truth: You DO have COVID-19.

* Test says you do have the virus---again, a correct decision!
* Test says you do not have the virus---this is a *false negative* (Type II error)


## Type I Error and Significance

A Type I error happens when you conclude that your results are statistically significant when, in reality, they happened purely by chance. Essentially, you had the misfortune of getting a random result that is *improbable* but not *impossible*.

The risk of committing this error is the significance level (α) you choose. That is the value that you set at the start of your study as the decision criteria for a *p*-value, like saying strong evidence is a *p*-value ≤ 0.05 (or $z$-score larger than 2).

As discussed before, significance level is commonly set at 0.05 (5%). This means that your results have *at most* a 5% chance of occurring if the null hypothesis is actually true.

Type II error depends on many factors, including the sample size and how close a false $H_0$ is to the truth. 

